{
  "hash": "e24dfcf09d380f741232b4cfe1edf418",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Single permutation tests\"\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-tip}\n## Learning outcomes\n\n**Questions**\n\n- How do we analyse data without distributional assumptions?\n\n**Objectives**\n\nPerform Monte Carlo permutation tests for:\n\n- Two samples of continuous data\n:::\n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n### Libraries\n### Functions\n\n## Python\n\n### Libraries\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n```\n:::\n\n\n### Functions\n:::\n:::\n\n## Purpose and aim\n\nIf we wish to test for a difference between two groups in the case where the assumptions of a two-sample t-test just aren’t met, then a two-sample permutation test procedure is appropriate. It is also appropriate even if the assumptions of a t-test are met, but in that case, it would be easier to just do the t-test.\n\nOne of the additional benefits of permutation test is that we aren’t just restricted to testing hypotheses about the means of the two groups. We can test hypotheses about absolutely anything we want! So, we could see if the ranges of the two groups differed significantly etc.\n\n## Data and hypotheses\n\nLet’s consider an experimental data set where we have measured the weights of two groups of 12 female mice (so 24 mice in total). One group of mice was given a perfectly normal diet (`control`) and the other group of mice was given a high fat diet for several months (`highfat`).\n\nWe want to test whether there is any difference in the mean weight of the two groups. We still need to specify the hypotheses:\n\n$H_0$: there is no difference in the means of the two groups\n\n$H_1$: there is a difference in the means of the two groups Let’s read in the data and look at it:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmice_weight <- read_csv(\"data/mice_weight.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mice_weight, aes(x = diet, y = weight)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.1)\n```\n\n::: {.cell-output-display}\n![](resampling-practical-permutation-single_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmice_weight_py = pd.read_csv(\"data/mice_weight.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n(ggplot(mice_weight_py,\n         aes(x = \"diet\",\n             y = \"weight\")) +\n     geom_boxplot() +\n     geom_jitter(width = 0.1))\n```\n\n::: {.cell-output-display}\n![](resampling-practical-permutation-single_files/figure-html/unnamed-chunk-7-1.png){width=614}\n:::\n:::\n\n\n:::\n\nLooking at the data, it appears that there might be a difference between the mean weight of the two groups. The weights of the mice on the `highfat` diet appears somewhat higher than on `control`, although there is quite some overlap between the data.\n\nThe medians (the horizontal lines in the boxes) are shifted - as are the boxes. So, the first thing we probably want to do is to calculate the exact difference in means:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmice_weight %>% \n  group_by(diet) %>% \n  summarise(mean_weight = mean(weight)) %>% \n  ungroup()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  diet    mean_weight\n  <chr>         <dbl>\n1 control        23.8\n2 highfat        26.8\n```\n\n\n:::\n:::\n\n\nWe'll want to use this difference later on, so we store it in a variable:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobs_diff_weight <- mice_weight %>% \n  group_by(diet) %>% \n  summarise(mean_weight = mean(weight)) %>% \n  ungroup() %>% \n  # calculate the difference in weight\n  # (there are many ways that you can do this)\n  pull(mean_weight) %>% \n  diff()\n\nobs_diff_weight\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.020833\n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nobs_diff_weight = (mice_weight_py\n                  .groupby('diet')['weight']\n                  .mean()\n                  .diff()\n                  .iloc[-1])\n```\n:::\n\n\nThis gives us an observed difference of:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nobs_diff_weight\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3.020833333333332\n```\n\n\n:::\n:::\n\n\n\n:::\n\nWhat we want to know is: is this difference unusual/big/statistically significant? Specifically, how likely would it be to get a difference this big if there were no difference between the two groups?\n\n## Permutation theory\n\nThe key idea behind permutation techniques is that if the null hypothesis is true, and there is no difference between the two groups then if I were to switch some of the mice from one group to the next then this wouldn’t change the difference between the groups too much. If on the other hand there actually is a difference between the groups (with one group having much higher weights than the other), then if I were to switch some mice between the groups then this should average out the two groups leading to a smaller difference in group means.\n\nSo, what we do is we shuffle the mice weights around lots and lots of times, calculating the difference between the group means each time. Once we have done this shuffling hundreds or thousands of times, we will have loads of possible values for the difference in the two group means. At this stage we can look at our actual difference (the one we calculated from our original data) and see how this compares to all of the simulated differences.\n\nWe can calculate how many of the simulated differences are bigger than our real difference and this proportion is exactly the p-value that we’re looking for!\n\nLet look at how to carry this out in practice.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nWe will be randomly shuffling our data around. So we set the `seed`, to aid reproducibility for the example.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseed <- 2602\n```\n:::\n\n\nWe need to approach this a bit logically, since we are going to iterate a process multiple times. We can break down the steps into the following:\n\n1. Define the number of permutations.\n2. Permute (randomly assign) the `diet` labels, without replacing.\n3. Calculate the *new difference* in means between the groups.\n4. Store the difference and repeat.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(seed)\n\n# Set the number of permutations\nreps <- 1000\n\n# Create a place to store the values\npermuted_stats <- tibble(permuted_diff = numeric(reps))\n\n# Loop through process\nfor(i in 1:reps){\n  # Get the data  \n  permuted_data <- mice_weight\n  \n  # Permute (reshuffle) the group labels\n  permuted_data$diet <- sample(permuted_data$diet)\n  \n  # Calculate the new group differences\n  permuted_diff <- permuted_data %>% \n  group_by(diet) %>% \n  summarise(mean_weight = mean(weight)) %>% \n  ungroup() %>% \n  pull(mean_weight) %>% \n  diff()\n  \n  # Store the calculated difference\n  permuted_stats$permuted_diff[i] <- permuted_diff\n}\n```\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nseed = 2602\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.random.seed(seed)\n\n# Set the number of permutations\nreps = 1000\n\n# Create a place to store the values\npermuted_stats = pd.DataFrame({'permuted_diff': [0] * reps})\n\n# Loop through process\nfor i in range(reps):\n    # Get the data\n    permuted_data_py = mice_weight_py\n    \n    # Permute the group labels\n    permuted_data_py['diet'] = np.random.permutation(permuted_data_py['diet'].values)\n    \n    # Calculate the new group difference\n    permuted_diff = (permuted_data_py \n    .groupby('diet')['weight']\n    .mean()\n    .diff()\n    .iloc[-1])\n\n    # Store the calculated difference\n    permuted_stats['permuted_diff'][i] = permuted_diff\n```\n:::\n\n\n:::\n\nWe can visualise the difference as a histogram:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(permuted_stats, aes(permuted_diff)) +\n  geom_histogram() +\n  geom_vline(xintercept = obs_diff_weight, colour = \"blue\", linewidth = 1)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](resampling-practical-permutation-single_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(ggplot(permuted_stats,\n        aes(x = \"permuted_diff\")) +\n     geom_histogram() +\n     geom_vline(xintercept = obs_diff_weight, colour = \"blue\", size = 1))\n```\n\n::: {.cell-output-display}\n![](resampling-practical-permutation-single_files/figure-html/unnamed-chunk-17-1.png){width=614}\n:::\n:::\n\n\n:::\n\nThe histogram is centred around zero, as we would expect: under the null hypothesis in this analysis there shouldn’t be any difference between the groups.\n\nThe blue vertical line shows us the value of our actual observed difference.\n\nWe can see that our observed difference is unlikely (because it’s out in the tails of the distribution rather than in the middle), but we want to be able to say exactly how unlikely. To do that we need to calculate the proportion of simulated differences that were bigger than our observed value. And, because we’re interested in a two-tailed test, we also need to include any simulated values that were less than -3.02 (so the lower tail).\n\nWe do this in the following steps:\n\n1. Count the number of occurrences where the permuted difference (`permuted_diff`) is larger than the observed weight difference (`obs_diff_weight`).\n2. Divide this by the number of times we permuted the data (`reps`)\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\npermuted_stats %>% \n  filter(abs(permuted_diff) > obs_diff_weight) %>% \n  nrow()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 51\n```\n\n\n:::\n:::\n\n\nIf we divide this number by 1000 (the number of permutations), we get a value of 0.051.\n\nIn this case we have fixed the random number generator. You might not have done that and normally you probably don't want to either. In that case you will get a slightly different value every time you run this. In order to get more precision on this p-value we will need to run more than 1,000 replicates.\n\nA good rule of thumb is that, for 1,000 replicates, a permutation test will return a p- value to within 1% of the actual value (so in this case the p-value is probably between 0.041 and 0.061). If we go to 10,000 replicates, then the error in the estimated p-value reduces to about 0.1%.\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlarger_diff = len(permuted_stats[permuted_stats['permuted_diff'].abs() > obs_diff_weight])\n\nlarger_diff\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n50\n```\n\n\n:::\n:::\n\n\nIf we divide this number by 1000 (the number of permutations), we get a value of 0.05.\n\nIn this case we have fixed the random number generator. You might not have done that and normally you probably don't want to either. In that case you will get a slightly different value every time you run this. In order to get more precision on this p-value we will need to run more than 1,000 replicates.\n\nA good rule of thumb is that, for 1,000 replicates, a permutation test will return a p- value to within 1% of the actual value (so in this case the p-value is probably between 0.04 and 0.06). If we go to 10,000 replicates, then the error in the estimated p-value reduces to about 0.1%.\n:::\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n## Python\n\n:::\n\n\n## Exercises\n\n### Title {#sec-exr_title}\n\n:::{.callout-exercise}\n\n\n{{< level 2 >}}\n\n\n\nFor this exercise we'll be using the data from `data/file.csv`.\n\n::: {.callout-answer collapse=\"true\"}\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n## Python\n\n:::\n:::\n:::\n\n## Summary\n\n::: {.callout-tip}\n#### Key points\n\n-\n- \n:::\n",
    "supporting": [
      "resampling-practical-permutation-single_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}