[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Resampling and simulation techniques",
    "section": "",
    "text": "Overview\nTraditional statistical testing make use of various distributions and assumptions. This often works well, allowing us to analyse our data using standardised tests - or slight variations on them.\nHowever, sometimes you might end up with data that are just weird and the standard tests or even the adaptations of them no longer work. This is where resampling and simulation techniques are very useful. Here we use either the original data or simulate new data to explore our hypothesis.\nThese topics rely on a mixture of statistical literacy and programming competencies. These materials are aimed to provide background and practical tools to address this.\nNote: The materials are under active development. The learning objectives in brackets will be addressed in further versions of these materials.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Resampling and simulation techniques",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nUnderstand which resampling techniques there are and when to use them.\nAnalyse data through permutation techniques\n(Bootstrapping)\n(Cross-validation)\n(Simulation data)\n\n\n\n\nTarget audience\nThis course is aimed at researchers and data analysts with an intermediate level of statistical and programming knowledge.\n\n\nPrerequisites\nConfident in the use of R / Python; basic knowledge of statistics (e.g. attended the Core statistics course).\n\n\n\nExercises\nExercises in these materials are labelled according to their level of difficulty:\n\n\n\n\n\n\n\nLevel\nDescription\n\n\n\n\n  \nExercises in level 1 are simpler and designed to get you familiar with the concepts and syntax covered in the course.\n\n\n  \nExercises in level 2 combine different concepts together and apply it to a given task.\n\n\n  \nExercises in level 3 require going beyond the concepts and syntax introduced to solve new problems.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "Resampling and simulation techniques",
    "section": "Authors",
    "text": "Authors\n\nAbout the authors:\n\nMartin van Rongen  \nAffiliation: Bioinformatics Training Facility, University of Cambridge\nRoles: writing - original draft, review & editing; conceptualisation; coding\nMatt Castle Affiliation: Bioinformatics Training Facility, University of Cambridge\nRoles: writing - review & editing; conceptualisation",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Data & Setup",
    "section": "",
    "text": "Data\nThe data used in these materials is provided as a zip file. Download and unzip the folder to your Desktop to follow along with the materials.\nDownload",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data & Setup</span>"
    ]
  },
  {
    "objectID": "setup.html#software",
    "href": "setup.html#software",
    "title": "Data & Setup",
    "section": "Software",
    "text": "Software\n\nR and RStudio\n\n\nWindows\nDownload and install all these using default options:\n\nR\nRTools\nRStudio\n\n\n\nMac OS\nDownload and install all these using default options:\n\nR\nRStudio\n\n\n\nLinux\n\nGo to the R installation folder and look at the instructions for your distribution.\nDownload the RStudio installer for your distribution and install it using your package manager.\n\n\n\n\n\nPython\nFor this course we’ll be using Visual Studio Code. This provides support for various programming languages (including Python and R). It works on Windows, MacOS and Linux. It’s also open-source and free.\nPlease refer to the installation instructions and make sure that you verify that Python code will run.\nA brief sequence of events:\n\nInstall Visual Studio Code\nInstall the VS Code Python extension\nInstall a Python interpreter\n\nWindows: install from Python.org or use the Microsoft Store\nMacOS: install the Homebrew package manager, then use this to install Python\nLinux: comes with Python 3, but needs pip to install additional packages",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data & Setup</span>"
    ]
  },
  {
    "objectID": "materials/resampling-intro.html",
    "href": "materials/resampling-intro.html",
    "title": "3  Background",
    "section": "",
    "text": "3.1 Permutation methods\nPermutation methods resample the original data, assuming the null hypothesis. They allow us to perform an exact statistical hypothesis test. For example, if we have data that compare the response of a drug between a control and treatment group, our null hypothesis would be that there is no difference between control and treatment.\nWhat that means is that, if this null hypothesis is true, we should be able to randomly reshuffle or assign the control or treatment labels to the various measurements. If there really isn’t a difference between the two groups then this should not lead to any changes compared to the observed difference in response.\nWe can do this many, many times. Let’s say we do this 1,000 times. We can then calculate how often the permuted differences exceed the observed difference, which gives us an exact p-value: the probability of observing our data, given the null hypothesis!",
    "crumbs": [
      "Resampling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "materials/resampling-intro.html#bootstrapping",
    "href": "materials/resampling-intro.html#bootstrapping",
    "title": "3  Background",
    "section": "3.2 Bootstrapping",
    "text": "3.2 Bootstrapping\nBootstrapping is a technique for estimating confidence intervals for parameter estimates. We effectively treat our data as if it was the parent distribution, draw samples from it and calculate the statistic of choice (the mean usually) using these sub-samples. If we repeat this process many times, we will eventually be able to construct a distribution for our sample statistic. This can be used to give us a confidence interval for our statistic.",
    "crumbs": [
      "Resampling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "materials/resampling-intro.html#cross-validation",
    "href": "materials/resampling-intro.html#cross-validation",
    "title": "3  Background",
    "section": "3.3 Cross-validation",
    "text": "3.3 Cross-validation\nCross-validation is at the heart of modern machine learning approaches but existed long before this technique became sexy/fashionable. You divide your data up into two sets: a training set that you use to fit your model and a testing set that you use to evaluate your model. This allows your model accuracy to be empirically measured. There are several variants of this technique (holdout, k-fold cross validation, leave-one-out-cross-validation (LOOCV), leave-p-out-cross-validation (LpOCV) etc.), all of which do essentially the same thing; the main difference between them being a trade-off between the amount of time it takes to perform versus the reliability of the method.\nWe’ll start with permutations techniques and cover bootstrapping and cross-validation in subsequent practicals.",
    "crumbs": [
      "Resampling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "materials/resampling-practical-permutation-single.html",
    "href": "materials/resampling-practical-permutation-single.html",
    "title": "4  Single permutation tests",
    "section": "",
    "text": "4.1 Libraries and functions",
    "crumbs": [
      "Resampling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single permutation tests</span>"
    ]
  },
  {
    "objectID": "materials/resampling-practical-permutation-single.html#libraries-and-functions",
    "href": "materials/resampling-practical-permutation-single.html#libraries-and-functions",
    "title": "4  Single permutation tests",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\nRPython\n\n\n\n4.1.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n\n\n4.1.2 Functions\n\n# Takes a random sample with or without replacement (default)\nbase::sample()\n\n\n\n\n\n4.1.3 Libraries\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# A Python package for scientific computing\nimport numpy as np\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Python module providing statistical functionality\nfrom scipy import stats\n\n\n\n4.1.4 Functions\n\n# Calculates the difference between two elements\npandas.DataFrame.diff()\n\n# Randomly permutes a sequence\nnumpy.random.permutation()\n\n# Calculates the interquartile range\nscipy.stats.iqr()",
    "crumbs": [
      "Resampling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single permutation tests</span>"
    ]
  },
  {
    "objectID": "materials/resampling-practical-permutation-single.html#purpose-and-aim",
    "href": "materials/resampling-practical-permutation-single.html#purpose-and-aim",
    "title": "4  Single permutation tests",
    "section": "4.2 Purpose and aim",
    "text": "4.2 Purpose and aim\nIf we wish to test for a difference between two groups in the case where the assumptions of a two-sample t-test just aren’t met, then a two-sample permutation test procedure is appropriate. It is also appropriate even if the assumptions of a t-test are met, but in that case, it would be easier to just do the t-test.\nOne of the additional benefits of permutation test is that we aren’t just restricted to testing hypotheses about the means of the two groups. We can test hypotheses about absolutely anything we want! So, we could see if the ranges of the two groups differed significantly etc.",
    "crumbs": [
      "Resampling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single permutation tests</span>"
    ]
  },
  {
    "objectID": "materials/resampling-practical-permutation-single.html#data-and-hypotheses",
    "href": "materials/resampling-practical-permutation-single.html#data-and-hypotheses",
    "title": "4  Single permutation tests",
    "section": "4.3 Data and hypotheses",
    "text": "4.3 Data and hypotheses\nLet’s consider an experimental data set where we have measured the weights of two groups of 12 female mice (so 24 mice in total). One group of mice was given a perfectly normal diet (control) and the other group of mice was given a high fat diet for several months (highfat).\nWe want to test whether there is any difference in the mean weight of the two groups. We still need to specify the hypotheses:\n\\(H_0\\): there is no difference in the means of the two groups\n\\(H_1\\): there is a difference in the means of the two groups Let’s read in the data and look at it:\n\nRPython\n\n\n\nmice_weight &lt;- read_csv(\"data/mice_weight.csv\")\n\n\nggplot(mice_weight, aes(x = diet, y = weight)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.1)\n\n\n\n\n\n\n\n\n\n\n\nmice_weight_py = pd.read_csv(\"data/mice_weight.csv\")\n\n\n(ggplot(mice_weight_py,\n         aes(x = \"diet\",\n             y = \"weight\")) +\n     geom_boxplot() +\n     geom_jitter(width = 0.1))\n\n\n\n\n\n\n\n\n\n\n\nLooking at the data, it appears that there might be a difference between the mean weight of the two groups. The weights of the mice on the highfat diet appears somewhat higher than on control, although there is quite some overlap between the data.\nThe medians (the horizontal lines in the boxes) are shifted - as are the boxes. So, the first thing we probably want to do is to calculate the exact difference in means:\n\nRPython\n\n\n\nmice_weight %&gt;% \n  group_by(diet) %&gt;% \n  summarise(mean_weight = mean(weight)) %&gt;% \n  ungroup()\n\n# A tibble: 2 × 2\n  diet    mean_weight\n  &lt;chr&gt;         &lt;dbl&gt;\n1 control        23.8\n2 highfat        26.8\n\n\nWe’ll want to use this difference later on, so we store it in a variable:\n\nobs_diff_weight &lt;- mice_weight %&gt;% \n  group_by(diet) %&gt;% \n  summarise(mean_weight = mean(weight)) %&gt;% \n  ungroup() %&gt;% \n  # calculate the difference in weight\n  # (there are many ways that you can do this)\n  pull(mean_weight) %&gt;% \n  diff()\n\nobs_diff_weight\n\n[1] 3.020833\n\n\n\n\n\nobs_diff_weight = (mice_weight_py\n                  .groupby('diet')['weight']\n                  .mean()\n                  .diff()\n                  .iloc[-1])\n\nThis gives us an observed difference of:\n\nobs_diff_weight\n\n3.020833333333332\n\n\n\n\n\nWhat we want to know is: is this difference unusual/big/statistically significant? Specifically, how likely would it be to get a difference this big if there were no difference between the two groups?",
    "crumbs": [
      "Resampling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single permutation tests</span>"
    ]
  },
  {
    "objectID": "materials/resampling-practical-permutation-single.html#permutation-theory",
    "href": "materials/resampling-practical-permutation-single.html#permutation-theory",
    "title": "4  Single permutation tests",
    "section": "4.4 Permutation theory",
    "text": "4.4 Permutation theory\nThe key idea behind permutation techniques is that if the null hypothesis is true, and there is no difference between the two groups then if I were to switch some of the mice from one group to the next then this wouldn’t change the difference between the groups too much. If on the other hand there actually is a difference between the groups (with one group having much higher weights than the other), then if I were to switch some mice between the groups then this should average out the two groups leading to a smaller difference in group means.\nSo, what we do is we shuffle the mice weights around lots and lots of times, calculating the difference between the group means each time. Once we have done this shuffling hundreds or thousands of times, we will have loads of possible values for the difference in the two group means. At this stage we can look at our actual difference (the one we calculated from our original data) and see how this compares to all of the simulated differences.\nWe can calculate how many of the simulated differences are bigger than our real difference and this proportion is exactly the p-value that we’re looking for!",
    "crumbs": [
      "Resampling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single permutation tests</span>"
    ]
  },
  {
    "objectID": "materials/resampling-practical-permutation-single.html#permutation-example",
    "href": "materials/resampling-practical-permutation-single.html#permutation-example",
    "title": "4  Single permutation tests",
    "section": "4.5 Permutation example",
    "text": "4.5 Permutation example\nLet look at how to carry this out in practice.\nWe need to approach this a bit logically, since we are going to iterate a process multiple times. We can break down the steps into the following:\n\nDefine the number of permutations.\nPermute (randomly assign) the diet labels, without replacing.\nCalculate the new difference in means between the groups.\nStore the difference and repeat.\n\n\n4.5.1 Permute the data\n\nRPython\n\n\nWe will be randomly shuffling our data around. So we set the seed, to aid reproducibility for the example.\n\nseed &lt;- 2602\n\n\nset.seed(seed)\n\n# Set the number of permutations\nreps &lt;- 1000\n\n# Create a place to store the values\npermuted_stats &lt;- tibble(permuted_diff = numeric(reps))\n\n# Loop through process\nfor(i in 1:reps){\n  # Get the data  \n  permuted_data &lt;- mice_weight\n  \n  # Permute (reshuffle) the group labels\n  permuted_data$diet &lt;- sample(permuted_data$diet)\n  \n  # Calculate the new group differences\n  permuted_diff &lt;- permuted_data %&gt;% \n  group_by(diet) %&gt;% \n  summarise(mean_weight = mean(weight)) %&gt;% \n  ungroup() %&gt;% \n  pull(mean_weight) %&gt;% \n  diff()\n  \n  # Store the calculated difference\n  permuted_stats$permuted_diff[i] &lt;- permuted_diff\n}\n\n\n\n\nseed = 2602\n\n\nnp.random.seed(seed)\n\n# Set the number of permutations\nreps = 1000\n\n# Create a place to store the values\npermuted_stats = pd.DataFrame({'permuted_diff': [0] * reps})\n\n# Loop through process\nfor i in range(reps):\n    # Get the data\n    permuted_data_py = mice_weight_py\n    \n    # Permute the group labels\n    permuted_data_py['diet'] = (np\n                               .random\n                               .permutation(permuted_data_py['diet']\n                               .values))\n    \n    # Calculate the new group difference\n    permuted_diff = (permuted_data_py \n    .groupby('diet')['weight']\n    .mean()\n    .diff()\n    .iloc[-1])\n\n    # Store the calculated difference\n    permuted_stats['permuted_diff'][i] = permuted_diff\n\n\n\n\n\n\n4.5.2 Comparing permuted values\nWe can visualise the difference as a histogram:\n\nRPython\n\n\n\nggplot(permuted_stats, aes(permuted_diff)) +\n  geom_histogram() +\n  geom_vline(xintercept = obs_diff_weight, colour = \"blue\", linewidth = 1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n(ggplot(permuted_stats,\n        aes(x = \"permuted_diff\")) +\n     geom_histogram() +\n     geom_vline(xintercept = obs_diff_weight, colour = \"blue\", size = 1))\n\n\n\n\n\n\n\n\n\n\n\nThe histogram is centred around zero, as we would expect: under the null hypothesis in this analysis there shouldn’t be any difference between the groups.\nThe blue vertical line shows us the value of our actual observed difference.\nWe can see that our observed difference is unlikely (because it’s out in the tails of the distribution rather than in the middle), but we want to be able to say exactly how unlikely. To do that we need to calculate the proportion of simulated differences that were bigger than our observed value. And, because we’re interested in a two-tailed test, we also need to include any simulated values that were less than -3.02 (so the lower tail).\n\n\n4.5.3 Calculating statistical significance\nIn essence, we are calculating the statistical significance by comparing our original data against the null hypothesis.\nWe do this in the following steps:\n\nCount the number of occurrences where the permuted difference (permuted_diff) is larger than the observed weight difference (obs_diff_weight).\nDivide this by the number of times we permuted the data (reps)\n\n\nRPython\n\n\n\npermuted_stats %&gt;% \n  filter(abs(permuted_diff) &gt; obs_diff_weight) %&gt;% \n  nrow()\n\n[1] 51\n\n\nIf we divide this number by 1000 (the number of permutations), we get a value of 0.051.\nIn this case we have fixed the random number generator. You might not have done that and normally you probably don’t want to either. In that case you will get a slightly different value every time you run this. In order to get more precision on this p-value we will need to run more than 1,000 replicates.\nA good rule of thumb is that, for 1,000 replicates, a permutation test will return a p-value to within 1% of the actual value (so in this case the p-value is probably between 0.041 and 0.061). If we go to 10,000 replicates, then the error in the estimated p-value reduces to about 0.1%.\n\n\n\nlarger_diff = len(permuted_stats[permuted_stats['permuted_diff'] \\\n                  .abs() &gt; obs_diff_weight])\n\nlarger_diff\n\n50\n\n\nIf we divide this number by 1000 (the number of permutations), we get a value of 0.05.\nIn this case we have fixed the random number generator. You might not have done that and normally you probably don’t want to either. In that case you will get a slightly different value every time you run this. In order to get more precision on this p-value we will need to run more than 1,000 replicates.\nA good rule of thumb is that, for 1,000 replicates, a permutation test will return a p-value to within 1% of the actual value (so in this case the p-value is probably between 0.04 and 0.06). If we go to 10,000 replicates, then the error in the estimated p-value reduces to about 0.1%.",
    "crumbs": [
      "Resampling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single permutation tests</span>"
    ]
  },
  {
    "objectID": "materials/resampling-practical-permutation-single.html#exercises",
    "href": "materials/resampling-practical-permutation-single.html#exercises",
    "title": "4  Single permutation tests",
    "section": "4.6 Exercises",
    "text": "4.6 Exercises\n\n4.6.1 Permuting IQR\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nFor this exercise we’ll again use the mice_weight data from data/mice_weight.csv.\nOne of the advantages of using permutation tests is that we’re not limited to just exploring the mean or median from our data. To practice this, we’ll explore differences in the interquartile range (IQR) between the control and highfat groups.\nQuestion: is there a significant difference in the IQR of weight between control and highfat mice?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nTo address the question, we do the following:\n\nLoad and visualise the data\nCalculate the observed IQR for both groups\nPermute the data\nCalculate how often the permuted IQR is larger than the observed\n\n\nRPython\n\n\n\nLoad and visualise the data\n\nmice_weight &lt;- read_csv(\"data/mice_weight.csv\")\n\nWe have visualised the data previously.\n\n\nObserved statistic\nTo calculate the interquartile range, we use the IQR() function. The IQR for each group is as follows:\n\nmice_weight %&gt;% \n  group_by(diet) %&gt;% \n  summarise(iqr_weight = IQR(weight))\n\n# A tibble: 2 × 2\n  diet    iqr_weight\n  &lt;chr&gt;        &lt;dbl&gt;\n1 control       5.04\n2 highfat       5.49\n\n\nThe observed difference in IQR is:\n\nobs_diff_iqr &lt;- mice_weight %&gt;% \n  group_by(diet) %&gt;% \n  summarise(iqr_weight = IQR(weight)) %&gt;% \n  pull(iqr_weight) %&gt;% \n  diff()\n\nobs_diff_iqr\n\n[1] 0.45\n\n\n\n\nPermute the data\n\nseed &lt;- 2602\n\n\nset.seed(seed)\n\n# Set the number of permutations\nreps &lt;- 1000\n\n# Create a place to store the values\npermuted_stats &lt;- tibble(permuted_diff = numeric(reps))\n\n# Loop through process\nfor(i in 1:reps){\n  # Get the data  \n  permuted_data &lt;- mice_weight\n  \n  # Permute (reshuffle) the group labels\n  permuted_data$diet &lt;- sample(permuted_data$diet)\n  \n  # Calculate the new group differences\n  permuted_diff &lt;- permuted_data %&gt;% \n  group_by(diet) %&gt;% \n  summarise(iqr_weight = IQR(weight)) %&gt;% \n  ungroup() %&gt;% \n  pull(iqr_weight) %&gt;% \n  diff()\n  \n  # Store the calculated difference\n  permuted_stats$permuted_diff[i] &lt;- permuted_diff\n}\n\nWe visualise these as follows:\n\nggplot(permuted_stats, aes(permuted_diff)) +\n  geom_histogram() +\n  geom_vline(xintercept = obs_diff_iqr, colour = \"blue\", linewidth = 1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nCalculate statistical significance\n\npermuted_stats %&gt;% \n  filter(abs(permuted_diff) &gt; obs_diff_iqr) %&gt;% \n  nrow()\n\n[1] 803\n\n\nDividing this by the number of permutations (1000) gives us a p-value of 0.803.\n\n\n\n\nLoad and visualise the data\n\nmice_weight_py = pd.read_csv(\"data/mice_weight.csv\")\n\nWe have visualised the data previously.\n\n\nObserved statistic\nWe use the iqr() function from scipy.stats:\n\nfrom scipy.stats import iqr\n\nobs_iqr = (mice_weight_py\n          .groupby('diet')['weight']\n          .agg(iqr))\n\nobs_iqr\n\ndiet\ncontrol    5.0375\nhighfat    5.4875\nName: weight, dtype: float64\n\n\nThis gives us an observed difference of:\n\nobs_diff_iqr = obs_iqr.diff().iloc[-1]\n\nobs_diff_iqr\n\n0.45000000000000284\n\n\n\n\nPermute the data\n\nseed = 2602\n\n\nnp.random.seed(seed)\n\n# Set the number of permutations\nreps = 1000\n\n# Create a place to store the values\npermuted_stats = pd.DataFrame({'permuted_diff': [0] * reps})\n\n# Loop through process\nfor i in range(reps):\n    # Get the data\n    permuted_data_py = mice_weight_py\n    \n    # Permute the group labels\n    permuted_data_py['diet'] = (np\n                               .random\n                               .permutation(permuted_data_py['diet']\n                               .values))\n    \n    # Calculate the new group difference\n    permuted_iqr = (permuted_data_py\n                   .groupby('diet')['weight']\n                   .agg(iqr))\n\n    permuted_diff = permuted_iqr.diff().iloc[-1]\n\n    # Store the calculated difference\n    permuted_stats['permuted_diff'][i] = permuted_diff\n\n\n(ggplot(permuted_stats,\n        aes(x = \"permuted_diff\")) +\n     geom_histogram() +\n     geom_vline(xintercept = obs_diff_iqr, colour = \"blue\", size = 1))\n\n\n\n\n\n\n\n\n\n\nCalculate the statistical significance\nHere we need to find all the values where the permuted IQR is smaller than -0.45 or larger than 0.45:\n\nlarger_diff = len(permuted_stats[permuted_stats['permuted_diff'] \\\n                  .abs() &gt; obs_diff_iqr])\n\nlarger_diff\n\n808\n\n\nIf we divide this number by 1000 (the number of permutations), we get a value of 0.808.\n\n\n\n\nThis analysis shows that there is no statistically significant difference between the interquartile range of weight for the two different diets.\n\n\n\n\n\n\nDifferences in IQR in R vs Python: would you like to know more?\n\n\n\n\n\nThe eagle-eyed amongst you might have noticed that the values calculated between R and Python are slightly different. Part of this is caused by the difference in how the random number generators work between the two languages (which we’re not going to go into) and part of it by the difference in how the IQR is calculated.\nIn R, the IQR() function uses a default method to calculate the quartiles (“type-7”), which excludes the smallest and largest 25% of the data when calculating the quartiles.\nIn Python, the scipy.stats.iqr() function calculates the interquartile range simply as the difference between the 75th and 25th percentiles.\nHence, some slight differences. If you’ve really got your mind set on making them more equivalent you can specify an extra argument in Python: rng. You can set it to include the middle 50% of the data: iqr(data, rng = (25, 75)).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.6.2 Rats - strange metrics\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nFor this exercise we’ll be using the data from data/rats_wheel.csv.\nThis data set contains information on the length of time that 24 rats were able to stay balanced on a rotating wheel. Half of the rats were assigned to the control group and the other half were given a dose of a centrally acting muscle relaxant. The animals were placed on a rotating cylinder and the length of time that each rat remained on the cylinder was measured, up to a maximum of 300 seconds.\nThe data set contains two variables: time and group.\nWhilst you could explore differences in means between these two groups, in this case an alternative statistic presents itself. When you look at the data you should notice that for the control group that all 12 rats manage to stay on the roller for the maximum 300 seconds, whereas in the treated group 5 out of the 12 fall off earlier.\nFor this exercise, instead of calculating the mean length of time for each group, you should calculate the proportion of rats that make it to 300s in each group and find the difference. This will be your statistic.\nAnswer the following questions:\n\nIs the proportion of rats that remain on the wheel the entire duration of the experiment is the same between each group? Use a permutation test to explore this.\nWhy would the difference in medians be a particularly useless statistic in this case?\nConsider the number of repetitions. What is a sensible number to apply and why?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nLoad and visualise the data.\n\nRPython\n\n\n\nrats_wheel &lt;- read_csv(\"data/rats_wheel.csv\")\n\nRows: 24 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): group\ndbl (1): time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nggplot(rats_wheel, aes(x = group, y = time)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.1)\n\n\n\n\n\n\n\n\n\n\n\nrats_wheel_py = pd.read_csv(\"data/rats_wheel.csv\")\n\n\n(ggplot(rats_wheel_py,\n         aes(x = \"group\",\n             y = \"time\")) +\n     geom_boxplot() +\n     geom_jitter(width = 0.1))\n\n\n\n\n\n\n\n\n\n\n\nWe can immediately see what the issue is with these data. All of the control subjects stayed on the wheel for 300s. If we would check the diagnostic plots for these data then it would not look very promising. For example, I am confident that the equality of variance assumption will not hold here!\n\nProportion of rats to full time\nSo, let’s do what we’re asked to do and calculate the proportion of rats that make it to 300s, for each group.\n\nRPython\n\n\nThere are many ways of doing this, but here is one:\n\nprop_rats &lt;- rats_wheel %&gt;% \n  group_by(group, time) %&gt;% \n  count() %&gt;% \n  group_by(group) %&gt;% \n  mutate(group_n = sum(n),\n         prop_rats = n / group_n) %&gt;% \n  filter(time == 300)\n\nprop_rats\n\n# A tibble: 2 × 5\n# Groups:   group [2]\n  group      time     n group_n prop_rats\n  &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt;   &lt;int&gt;     &lt;dbl&gt;\n1 control     300    12      12     1    \n2 treatment   300     7      12     0.583\n\n\nNext, we calculate the difference in the proportion of rats that make it to 300s, between the two groups.\n\nobs_diff_prop &lt;- prop_rats %&gt;% \n  pull(prop_rats) %&gt;% \n  diff()\n\nobs_diff_prop\n\n[1] -0.4166667\n\n\n\n\nThere are many ways of doing this, but here’s one:\n\n# Calculate the total number of observations in each group\nrats_wheel_py['group_n'] = rats_wheel_py.groupby('group')['group'].transform('size')\n\n# Count the number of occurrences for each unique time point\n# and keep the total group count\nprop_rats_py = rats_wheel_py.groupby(['group', 'group_n', 'time']).size().reset_index(name = 'n')\n\n# Calculate the proportion of rats that make it to each time point\nprop_rats_py['prop_rats'] = prop_rats_py['n'] / prop_rats_py['group_n']\n\n# Filter for the 300s time point\nprop_rats_py = prop_rats_py[prop_rats_py['time'] == 300]\n\nprop_rats_py\n\n       group  group_n  time   n  prop_rats\n0    control       12   300  12   1.000000\n6  treatment       12   300   7   0.583333\n\n\nNext, we calculate the difference in the proportion of rats that make it to 300s, between the two groups.\n\nobs_diff_prop = prop_rats_py['prop_rats'].diff().iloc[-1]\n\nobs_diff_prop\n\n-0.41666666666666663\n\n\n\n\n\n\n\nPermute the data\nNow we’ve got that out of the way, we can permute our data. We’re reshuffling the group labels randomly, then recalculating the permuted proportional difference at time point 300s.\n\nRPython\n\n\n\nseed &lt;- 2602\n\n\nset.seed(seed)\n\n# Set the number of permutations\nreps &lt;- 1000\n\n# Create a place to store the values\npermuted_stats &lt;- tibble(permuted_diff = numeric(reps))\n\n# Loop through process\nfor(i in 1:reps){\n  # Get the data  \n  permuted_data &lt;- rats_wheel\n  \n  # Permute (reshuffle) the group labels\n  permuted_data$group &lt;- sample(permuted_data$group)\n  \n  # Calculate the new proportional differences\n  \n  permuted_diff &lt;- permuted_data %&gt;% \n  group_by(group, time) %&gt;% \n  count() %&gt;% \n  group_by(group) %&gt;% \n  mutate(group_n = sum(n),\n         prop_rats = n / group_n) %&gt;% \n  filter(time == 300) %&gt;% \n  pull(prop_rats) %&gt;% \n  diff()\n  \n  # Store the calculated difference\n  permuted_stats$permuted_diff[i] &lt;- permuted_diff\n}\n\n\n\n\nseed = 2602\n\n\nnp.random.seed(seed)\n\n# Set the number of permutations\nreps = 1000\n\n# Create a place to store the values\npermuted_stats = pd.DataFrame({'permuted_diff': [0] * reps})\n\n# Loop through process\nfor i in range(reps):\n    # Get the data\n    permuted_data_py = rats_wheel_py\n    \n    # Permute the group labels\n    permuted_data_py['group'] = (np\n                               .random\n                               .permutation(permuted_data_py['group']\n                               .values))\n    \n    # Calculate the new group difference\n    \n    # Calculate the total number of observations in each group\n    permuted_data_py['group_n'] = \\\n    permuted_data_py.groupby('group')['group'].transform('size')\n\n    # Count the number of occurrences for each unique time point\n    # and keep the total group count\n    prop_rats_py = \\\n    permuted_data_py.groupby(['group', 'group_n', 'time']) \\\n    .size().reset_index(name = 'n')\n\n    # Calculate the proportion of rats that make it to each time point\n    prop_rats_py['prop_rats'] = \\\n    prop_rats_py['n'] / prop_rats_py['group_n']\n\n    # Filter for the 300s time point\n    prop_rats_py = prop_rats_py[prop_rats_py['time'] == 300]\n\n    permuted_diff = prop_rats_py['prop_rats'].diff().iloc[-1]\n\n    # Store the calculated difference\n    permuted_stats['permuted_diff'][i] = permuted_diff\n\n\n\n\n\n\nCompare against observed difference\nWe can now compare the permuted values. There are a limited number of unique permuted differences (6, to be precise), so we’re limited the number of bins to this.\n\nRPython\n\n\n\nggplot(permuted_stats, aes(permuted_diff)) +\n  geom_histogram(bins = 6) +\n  geom_vline(xintercept = obs_diff_prop, colour = \"blue\", linewidth = 1) +\n  geom_vline(xintercept = abs(obs_diff_prop), colour = \"blue\", linewidth = 1)\n\n\n\n\n\n\n\n\n\n\n\n(ggplot(permuted_stats,\n        aes(x = \"permuted_diff\")) +\n     geom_histogram(bins = 6) +\n     geom_vline(xintercept = obs_diff_prop, colour = \"blue\", size = 1) +\n     geom_vline(xintercept = abs(obs_diff_prop), colour = \"blue\", size = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical evaluation\nWe can now answer the question if the proportion of rats that make it to full time is different between the groups. We do this by comparing the number of occurrences in the resampled data against the original data. How many times is the difference in proportion larger than the observed difference in proportion? Remember, we are doing a two-tailed test, so we need to get the values on either side of the observed proportion.\nIn this case the observed difference in proportion between control and treatment is negative, which we need to take into account.\n\nRPython\n\n\n\npermuted_stats %&gt;% \n  filter(permuted_diff &lt; obs_diff_prop |\n           permuted_diff &gt; abs(obs_diff_prop)) %&gt;% \n  nrow()\n\n[1] 0\n\n\n\n\n\nlen(permuted_stats[(permuted_stats['permuted_diff'] &lt; obs_diff_prop) | (permuted_stats['permuted_diff'] &gt; abs(obs_diff_prop))])\n\n0\n\n\n\n\n\nWe find that none of the permuted differences in proportion between control and treatment that make it to 300s is outside the one we observed. This means that it is extremely unlikely that we’d find these data, if there is indeed no difference between the two groups.\n\n\nUsing the median or not\nTo answer the second question: the median is a particularly useless statistic to use with these data because there is no variation in the measurements for the control group. All of the values are 300s, meaning you can’t find a value where 50% of the data is on one side of it and 50% of the data is on the other!\n\n\nNumber of repetitions\nThis then brings us to the third question: the number of repetitions. In the worked answer we’ve used 1,000 repetitions. However, we have two groups with only 12 observations. This means we’re limited in the number of unique combinations we can find.\nTo be more exact, we’re limited to:\n\\(C(n, k) = \\frac{n!}{k!(n-k)!}\\)\nwhere\n\\(n!\\) (n factorial) is the product of all positive integers up to \\(n\\). \\(k!\\) (k factorial) is the product of all positive integers up to \\(k\\).\nFor our data set this comes down to:\n\\(C(2 \\times 12, 2) = \\frac{(2 \\times 12)!}{2!(2 \\times 12 - 2)!}\\)\n\\(C(24, 2) = \\frac{24!}{2! \\times 22!}\\)\nWe can calculate this as follows:\n\nRPython\n\n\n\nn &lt;- 24\nk &lt;- 2\n\n# Calculate the number of unique combinations\nfactorial(n) / (factorial(k) * factorial(n - k))\n\n[1] 276\n\n\n\n\n\nimport math\n\nn = 24\nk = 2\n\nmath.factorial(n) // (math.factorial(k) * math.factorial(n - k))\n\n276\n\n\n\n\n\nThis means we can create 276 unique combinations.\nIf we run the analysis again using this number, then our output looks like:\n\n\n\n\n\n\n\n\n\nThis makes a quantitative difference: when we look at the frequency (count) of the permuted differences in proportion, it’s clear that the numbers are lower than when we permuted the data 1,000 times. This is because when we permuted more times than there were unique combinations, the same answer was calculated multiple times.",
    "crumbs": [
      "Resampling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single permutation tests</span>"
    ]
  },
  {
    "objectID": "materials/resampling-practical-permutation-single.html#summary",
    "href": "materials/resampling-practical-permutation-single.html#summary",
    "title": "4  Single permutation tests",
    "section": "4.7 Summary",
    "text": "4.7 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nWe can use resampled data to draw conclusions around how likely it is our original data would occur, given the null hypothesis.\nWe can calculate statistical significance using this approach.",
    "crumbs": [
      "Resampling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Single permutation tests</span>"
    ]
  }
]