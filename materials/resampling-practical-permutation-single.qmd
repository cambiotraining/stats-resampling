---
title: "Single permutation tests"
---

```{r}
#| echo: false
#| message: false
#| results: hide
source(file = "setup_files/setup.R")
```

```{python}
#| echo: false
#| message: false
import shutup;shutup.please()
exec(open('setup_files/setup.py').read())
```

::: {.callout-tip}
## Learning outcomes

**Questions**

- How do we analyse data without distributional assumptions?

**Objectives**

Perform Monte Carlo permutation tests for:

- Single predictors
:::

## Libraries and functions

::: {.callout-note collapse="true"}
## Click to expand

::: {.panel-tabset group="language"}
## R

### Libraries

```{r}
#| eval: false
# A collection of R packages designed for data science
library(tidyverse)
```

### Functions

```{r}
#| eval: false
#| warning: false
# Takes a random sample with or without replacement (default)
base::sample()
```

## Python

### Libraries

```{python}
#| eval: false
# A Python data analysis and manipulation tool
import pandas as pd

# A Python package for scientific computing
import numpy as np

# Python equivalent of `ggplot2`
from plotnine import *

# Python module providing statistical functionality
from scipy import stats
```

### Functions

```{python}
#| eval: false
# Calculates the difference between two elements
pandas.DataFrame.diff()

# Randomly permutes a sequence
numpy.random.permutation()

# Calculates the interquartile range
scipy.stats.iqr()
```
:::
:::

## Purpose and aim

If we wish to test for a difference between two groups in the case where the assumptions of a two-sample t-test just aren’t met, then a two-sample permutation test procedure is appropriate. It is also appropriate even if the assumptions of a t-test are met, but in that case, it would be easier to just do the t-test.

One of the additional benefits of permutation test is that we aren’t just restricted to testing hypotheses about the means of the two groups. We can test hypotheses about absolutely anything we want! So, we could see if the ranges of the two groups differed significantly etc.

## Data and hypotheses

Let’s consider an experimental data set where we have measured the weights of two groups of 12 female mice (so 24 mice in total). One group of mice was given a perfectly normal diet (`control`) and the other group of mice was given a high fat diet for several months (`highfat`).

We want to test whether there is any difference in the mean weight of the two groups. We still need to specify the hypotheses:

$H_0$: there is no difference in the means of the two groups

$H_1$: there is a difference in the means of the two groups Let’s read in the data and look at it:

::: {.panel-tabset group="language"}
## R

```{r}
#| message: false
mice_weight <- read_csv("data/mice_weight.csv")
```

```{r}
ggplot(mice_weight, aes(x = diet, y = weight)) +
  geom_boxplot() +
  geom_jitter(width = 0.1)
```

## Python

```{python}
mice_weight_py = pd.read_csv("data/mice_weight.csv")
```

```{python}
#| results: hide
(ggplot(mice_weight_py,
         aes(x = "diet",
             y = "weight")) +
     geom_boxplot() +
     geom_jitter(width = 0.1))
```

:::

Looking at the data, it appears that there might be a difference between the mean weight of the two groups. The weights of the mice on the `highfat` diet appears somewhat higher than on `control`, although there is quite some overlap between the data.

The medians (the horizontal lines in the boxes) are shifted - as are the boxes. So, the first thing we probably want to do is to calculate the exact difference in means:

::: {.panel-tabset group="language"}
## R

```{r}
mice_weight %>% 
  group_by(diet) %>% 
  summarise(mean_weight = mean(weight)) %>% 
  ungroup()
```

We'll want to use this difference later on, so we store it in a variable:

```{r}
obs_diff_weight <- mice_weight %>% 
  group_by(diet) %>% 
  summarise(mean_weight = mean(weight)) %>% 
  ungroup() %>% 
  # calculate the difference in weight
  # (there are many ways that you can do this)
  pull(mean_weight) %>% 
  diff()

obs_diff_weight
```

## Python

```{python}
obs_diff_weight = (mice_weight_py
                  .groupby('diet')['weight']
                  .mean()
                  .diff()
                  .iloc[-1])
```

This gives us an observed difference of:

```{python}
obs_diff_weight
```


:::

What we want to know is: is this difference unusual/big/statistically significant? Specifically, how likely would it be to get a difference this big if there were no difference between the two groups?

## Permutation theory

The key idea behind permutation techniques is that if the null hypothesis is true, and there is no difference between the two groups then if I were to switch some of the mice from one group to the next then this wouldn’t change the difference between the groups too much. If on the other hand there actually is a difference between the groups (with one group having much higher weights than the other), then if I were to switch some mice between the groups then this should average out the two groups leading to a smaller difference in group means.

So, what we do is we shuffle the mice weights around lots and lots of times, calculating the difference between the group means each time. Once we have done this shuffling hundreds or thousands of times, we will have loads of possible values for the difference in the two group means. At this stage we can look at our actual difference (the one we calculated from our original data) and see how this compares to all of the simulated differences.

We can calculate how many of the simulated differences are bigger than our real difference and this proportion is exactly the p-value that we’re looking for!

## Permutation example

Let look at how to carry this out in practice.

We need to approach this a bit logically, since we are going to iterate a process multiple times. We can break down the steps into the following:

1. Define the number of permutations.
2. Permute (randomly assign) the `diet` labels, without replacing.
3. Calculate the *new difference* in means between the groups.
4. Store the difference and repeat.

### Permute the data

::: {.panel-tabset group="language"}
## R

We will be randomly shuffling our data around. So we set the `seed`, to aid reproducibility for the example.

```{r}
seed <- 2602
```

```{r}
set.seed(seed)

# Set the number of permutations
reps <- 1000

# Create a place to store the values
permuted_stats <- tibble(permuted_diff = numeric(reps))

# Loop through process
for(i in 1:reps){
  # Get the data  
  permuted_data <- mice_weight
  
  # Permute (reshuffle) the group labels
  permuted_data$diet <- sample(permuted_data$diet)
  
  # Calculate the new group differences
  permuted_diff <- permuted_data %>% 
  group_by(diet) %>% 
  summarise(mean_weight = mean(weight)) %>% 
  ungroup() %>% 
  pull(mean_weight) %>% 
  diff()
  
  # Store the calculated difference
  permuted_stats$permuted_diff[i] <- permuted_diff
}
```

## Python

```{python}
seed = 2602
```

```{python}
np.random.seed(seed)

# Set the number of permutations
reps = 1000

# Create a place to store the values
permuted_stats = pd.DataFrame({'permuted_diff': [0] * reps})

# Loop through process
for i in range(reps):
    # Get the data
    permuted_data_py = mice_weight_py
    
    # Permute the group labels
    permuted_data_py['diet'] = (np
                               .random
                               .permutation(permuted_data_py['diet']
                               .values))
    
    # Calculate the new group difference
    permuted_diff = (permuted_data_py 
    .groupby('diet')['weight']
    .mean()
    .diff()
    .iloc[-1])

    # Store the calculated difference
    permuted_stats['permuted_diff'][i] = permuted_diff

```

:::

### Comparing permuted values

We can visualise the difference as a histogram:

::: {.panel-tabset group="language"}
## R

```{r}
ggplot(permuted_stats, aes(permuted_diff)) +
  geom_histogram() +
  geom_vline(xintercept = obs_diff_weight, colour = "blue", linewidth = 1)
```

## Python

```{python}
#| results: hide
(ggplot(permuted_stats,
        aes(x = "permuted_diff")) +
     geom_histogram() +
     geom_vline(xintercept = obs_diff_weight, colour = "blue", size = 1))
```

:::

The histogram is centred around zero, as we would expect: under the null hypothesis in this analysis there shouldn’t be any difference between the groups.

The blue vertical line shows us the value of our actual observed difference.

We can see that our observed difference is unlikely (because it’s out in the tails of the distribution rather than in the middle), but we want to be able to say exactly how unlikely. To do that we need to calculate the proportion of simulated differences that were bigger than our observed value. And, because we’re interested in a two-tailed test, we also need to include any simulated values that were less than `r round(-(obs_diff_weight), 2)` (so the lower tail).

### Calculating statistical significance

In essence, we are calculating the *statistical significance* by comparing our original data against the null hypothesis.

We do this in the following steps:

1. Count the number of occurrences where the permuted difference (`permuted_diff`) is larger than the observed weight difference (`obs_diff_weight`).
2. Divide this by the number of times we permuted the data (`reps`)

::: {.panel-tabset group="language"}
## R

```{r}
permuted_stats %>% 
  filter(abs(permuted_diff) > obs_diff_weight) %>% 
  nrow()
```

If we divide this number by `r reps` (the number of permutations), we get a value of `r round(permuted_stats %>% filter(abs(permuted_diff) > obs_diff_weight) %>% nrow() / reps, 3)`.

In this case we have fixed the random number generator. You might not have done that and normally you probably don't want to either. In that case you will get a slightly different value every time you run this. In order to get more precision on this p-value we will need to run more than 1,000 replicates.

A good rule of thumb is that, for 1,000 replicates, a permutation test will return a p-value to within 1% of the actual value (so in this case the p-value is probably between `r round(permuted_stats %>% filter(abs(permuted_diff) > obs_diff_weight) %>% nrow() / reps - 0.01, 3)` and `r round(permuted_stats %>% filter(abs(permuted_diff) > obs_diff_weight) %>% nrow() / reps + 0.01, 3)`). If we go to 10,000 replicates, then the error in the estimated p-value reduces to about 0.1%.

## Python

```{python}
larger_diff = len(permuted_stats[permuted_stats['permuted_diff'] \
                  .abs() > obs_diff_weight])

larger_diff
```

If we divide this number by `r py$reps` (the number of permutations), we get a value of `r round(py$permuted_stats %>% filter(abs(permuted_diff) > obs_diff_weight) %>% nrow() / reps, 3)`.

In this case we have fixed the random number generator. You might not have done that and normally you probably don't want to either. In that case you will get a slightly different value every time you run this. In order to get more precision on this p-value we will need to run more than 1,000 replicates.

A good rule of thumb is that, for 1,000 replicates, a permutation test will return a p-value to within 1% of the actual value (so in this case the p-value is probably between `r round(py$permuted_stats %>% filter(abs(permuted_diff) > obs_diff_weight) %>% nrow() / reps - 0.01, 3)` and `r round(py$permuted_stats %>% filter(abs(permuted_diff) > obs_diff_weight) %>% nrow() / reps + 0.01, 3)`). If we go to 10,000 replicates, then the error in the estimated p-value reduces to about 0.1%.
:::

## Exercises

### Permuting IQR {#sec-exr_iqr}

:::{.callout-exercise}

{{< level 2 >}}

For this exercise we'll again use the `mice_weight` data from `data/mice_weight.csv`.

One of the advantages of using permutation tests is that we're not limited to just exploring the mean or median from our data. To practice this, we'll explore differences in the interquartile range (IQR) between the `control` and `highfat` groups.

Question: is there a significant difference in the IQR of `weight` between `control` and `highfat` mice?

::: {.callout-answer collapse="true"}

To address the question, we do the following:

1. Load and visualise the data
2. Calculate the observed IQR for both groups
3. Permute the data
4. Calculate how often the permuted IQR is larger than the observed

::: {.panel-tabset group="language"}
## R

#### Load and visualise the data

```{r}
#| message: false
mice_weight <- read_csv("data/mice_weight.csv")
```

We have visualised the data previously.

#### Observed statistic

To calculate the interquartile range, we use the `IQR()` function. The IQR for each group is as follows:

```{r}
mice_weight %>% 
  group_by(diet) %>% 
  summarise(iqr_weight = IQR(weight))
```

The observed difference in IQR is:

```{r}
obs_diff_iqr <- mice_weight %>% 
  group_by(diet) %>% 
  summarise(iqr_weight = IQR(weight)) %>% 
  pull(iqr_weight) %>% 
  diff()

obs_diff_iqr
```

#### Permute the data

```{r}
seed <- 2602
```

```{r}
set.seed(seed)

# Set the number of permutations
reps <- 1000

# Create a place to store the values
permuted_stats <- tibble(permuted_diff = numeric(reps))

# Loop through process
for(i in 1:reps){
  # Get the data  
  permuted_data <- mice_weight
  
  # Permute (reshuffle) the group labels
  permuted_data$diet <- sample(permuted_data$diet)
  
  # Calculate the new group differences
  permuted_diff <- permuted_data %>% 
  group_by(diet) %>% 
  summarise(iqr_weight = IQR(weight)) %>% 
  ungroup() %>% 
  pull(iqr_weight) %>% 
  diff()
  
  # Store the calculated difference
  permuted_stats$permuted_diff[i] <- permuted_diff
}
```

We visualise these as follows:

```{r}
ggplot(permuted_stats, aes(permuted_diff)) +
  geom_histogram() +
  geom_vline(xintercept = obs_diff_iqr, colour = "blue", linewidth = 1)
```
#### Calculate statistical significance

```{r}
permuted_stats %>% 
  filter(abs(permuted_diff) > obs_diff_iqr) %>% 
  nrow()
```

Dividing this by the number of permutations (`r reps`) gives us a p-value of `r permuted_stats %>% filter(abs(permuted_diff) > obs_diff_iqr) %>% nrow() / reps`.

## Python

#### Load and visualise the data

```{python}
mice_weight_py = pd.read_csv("data/mice_weight.csv")
```

We have visualised the data previously.

#### Observed statistic

We use the `iqr()` function from `scipy.stats`:

```{python}
from scipy.stats import iqr

obs_iqr = (mice_weight_py
          .groupby('diet')['weight']
          .agg(iqr))

obs_iqr
```

This gives us an observed difference of:

```{python}
obs_diff_iqr = obs_iqr.diff().iloc[-1]

obs_diff_iqr
```

#### Permute the data

```{python}
seed = 2602
```

```{python}
np.random.seed(seed)

# Set the number of permutations
reps = 1000

# Create a place to store the values
permuted_stats = pd.DataFrame({'permuted_diff': [0] * reps})

# Loop through process
for i in range(reps):
    # Get the data
    permuted_data_py = mice_weight_py
    
    # Permute the group labels
    permuted_data_py['diet'] = (np
                               .random
                               .permutation(permuted_data_py['diet']
                               .values))
    
    # Calculate the new group difference
    permuted_iqr = (permuted_data_py
                   .groupby('diet')['weight']
                   .agg(iqr))

    permuted_diff = permuted_iqr.diff().iloc[-1]

    # Store the calculated difference
    permuted_stats['permuted_diff'][i] = permuted_diff
```

```{python}
#| results: hide
(ggplot(permuted_stats,
        aes(x = "permuted_diff")) +
     geom_histogram() +
     geom_vline(xintercept = obs_diff_iqr, colour = "blue", size = 1))
```

#### Calculate the statistical significance

Here we need to find all the values where the permuted IQR is *smaller* than `r -(py$obs_diff_iqr)` or *larger* than `r py$obs_diff_iqr %>% abs()`:

```{python}
larger_diff = len(permuted_stats[permuted_stats['permuted_diff'] \
                  .abs() > obs_diff_iqr])

larger_diff
```


If we divide this number by `r py$reps` (the number of permutations), we get a value of `r round(py$permuted_stats %>% filter(abs(permuted_diff) > abs(py$obs_diff_iqr)) %>% nrow() / reps, 3)`.

:::

This analysis shows that there is no statistically significant difference between the interquartile range of weight for the two different diets.


:::{.callout-note collapse=true}
## Differences in IQR in R vs Python: would you like to know more?

The eagle-eyed amongst you might have noticed that the values calculated between R and Python are slightly different. Part of this is caused by the difference in how the random number generators work between the two languages (which we're not going to go into) and part of it by the difference in how the IQR is calculated.

In R, the `IQR()` function uses a default method to calculate the quartiles ("type-7"), which excludes the smallest and largest 25% of the data when calculating the quartiles.

In Python, the `scipy.stats.iqr()` function calculates the interquartile range simply as the difference between the 75th and 25th percentiles.

Hence, some slight differences. If you've *really* got your mind set on making them more equivalent you can specify an extra argument in Python: `rng`. You can set it to include the middle 50% of the data: `iqr(data, rng = (25, 75))`.
:::

:::
:::

### Rats - strange metrics {#sec-exr_ratswheel}

:::{.callout-exercise}

{{< level 3 >}}

For this exercise we'll be using the data from `data/rats_wheel.csv`.

This data set contains information on the length of time that 24 rats were able to stay balanced on a rotating wheel. Half of the rats were assigned to the control group and the other half were given a dose of a centrally acting muscle relaxant. The animals were placed on a rotating cylinder and the length of time that each rat remained on the cylinder was measured, up to a maximum of 300 seconds.

The data set contains two variables: `time` and `group`.

Whilst you could explore differences in means between these two groups, in this case an alternative statistic presents itself. When you look at the data you should notice that for the control group that all 12 rats manage to stay on the roller for the maximum 300 seconds, whereas in the treated group 5 out of the 12 fall off earlier.

For this exercise, instead of calculating the mean length of time for each group, you should calculate the proportion of rats that make it to 300s in each group and find the difference. This will be your statistic.

Answer the following questions:

1. Is the proportion of rats that remain on the wheel the entire duration of the experiment is the same between each group? Use a permutation test to explore this.
2. Why would the difference in medians be a particularly useless statistic in this case?
3. Consider the number of repetitions. What is a sensible number to apply and why?

::: {.callout-answer collapse="true"}

Load and visualise the data.

::: {.panel-tabset group="language"}
## R

```{r}
rats_wheel <- read_csv("data/rats_wheel.csv")
```

```{r}
ggplot(rats_wheel, aes(x = group, y = time)) +
  geom_boxplot() +
  geom_jitter(width = 0.1)
```


## Python

```{python}
rats_wheel_py = pd.read_csv("data/rats_wheel.csv")
```

```{python}
#| results: hide
(ggplot(rats_wheel_py,
         aes(x = "group",
             y = "time")) +
     geom_boxplot() +
     geom_jitter(width = 0.1))
```

:::

We can immediately see what the issue is with these data. All of the `control` subjects stayed on the wheel for 300s. If we would check the diagnostic plots for these data then it would not look very promising. For example, I am confident that the equality of variance assumption will not hold here!

#### Proportion of rats to full time

So, let's do what we're asked to do and calculate the proportion of rats that make it to 300s, for each group.

::: {.panel-tabset group="language"}
## R

There are many ways of doing this, but here is one:

```{r}
prop_rats <- rats_wheel %>% 
  group_by(group, time) %>% 
  count() %>% 
  group_by(group) %>% 
  mutate(group_n = sum(n),
         prop_rats = n / group_n) %>% 
  filter(time == 300)

prop_rats
```

Next, we calculate the *difference* in the proportion of rats that make it to 300s, between the two groups.

```{r}
obs_diff_prop <- prop_rats %>% 
  pull(prop_rats) %>% 
  diff()

obs_diff_prop
```

## Python

There are many ways of doing this, but here's one:

```{python}
# Calculate the total number of observations in each group
rats_wheel_py['group_n'] = rats_wheel_py.groupby('group')['group'].transform('size')

# Count the number of occurrences for each unique time point
# and keep the total group count
prop_rats_py = rats_wheel_py.groupby(['group', 'group_n', 'time']).size().reset_index(name = 'n')

# Calculate the proportion of rats that make it to each time point
prop_rats_py['prop_rats'] = prop_rats_py['n'] / prop_rats_py['group_n']

# Filter for the 300s time point
prop_rats_py = prop_rats_py[prop_rats_py['time'] == 300]

prop_rats_py
```

Next, we calculate the *difference* in the proportion of rats that make it to 300s, between the two groups.

```{python}
obs_diff_prop = prop_rats_py['prop_rats'].diff().iloc[-1]

obs_diff_prop
```
:::

#### Permute the data

Now we've got that out of the way, we can permute our data. We're reshuffling the `group` labels randomly, then recalculating the permuted proportional difference at time point 300s.

::: {.panel-tabset group="language"}
## R

```{r}
seed <- 2602
```

```{r}
set.seed(seed)

# Set the number of permutations
reps <- 1000

# Create a place to store the values
permuted_stats <- tibble(permuted_diff = numeric(reps))

# Loop through process
for(i in 1:reps){
  # Get the data  
  permuted_data <- rats_wheel
  
  # Permute (reshuffle) the group labels
  permuted_data$group <- sample(permuted_data$group)
  
  # Calculate the new proportional differences
  
  permuted_diff <- permuted_data %>% 
  group_by(group, time) %>% 
  count() %>% 
  group_by(group) %>% 
  mutate(group_n = sum(n),
         prop_rats = n / group_n) %>% 
  filter(time == 300) %>% 
  pull(prop_rats) %>% 
  diff()
  
  # Store the calculated difference
  permuted_stats$permuted_diff[i] <- permuted_diff
}
```

## Python

```{python}
seed = 2602
```

```{python}
np.random.seed(seed)

# Set the number of permutations
reps = 1000

# Create a place to store the values
permuted_stats = pd.DataFrame({'permuted_diff': [0] * reps})

# Loop through process
for i in range(reps):
    # Get the data
    permuted_data_py = rats_wheel_py
    
    # Permute the group labels
    permuted_data_py['group'] = (np
                               .random
                               .permutation(permuted_data_py['group']
                               .values))
    
    # Calculate the new group difference
    
    # Calculate the total number of observations in each group
    permuted_data_py['group_n'] = \
    permuted_data_py.groupby('group')['group'].transform('size')

    # Count the number of occurrences for each unique time point
    # and keep the total group count
    prop_rats_py = \
    permuted_data_py.groupby(['group', 'group_n', 'time']) \
    .size().reset_index(name = 'n')

    # Calculate the proportion of rats that make it to each time point
    prop_rats_py['prop_rats'] = \
    prop_rats_py['n'] / prop_rats_py['group_n']

    # Filter for the 300s time point
    prop_rats_py = prop_rats_py[prop_rats_py['time'] == 300]

    permuted_diff = prop_rats_py['prop_rats'].diff().iloc[-1]

    # Store the calculated difference
    permuted_stats['permuted_diff'][i] = permuted_diff
```

:::

#### Compare against observed difference

We can now compare the permuted values. There are a limited number of unique permuted differences (`r permuted_stats %>% distinct(permuted_diff) %>% nrow()`, to be precise), so we're limited the number of bins to this.

::: {.panel-tabset group="language"}
## R

```{r}
ggplot(permuted_stats, aes(permuted_diff)) +
  geom_histogram(bins = 6) +
  geom_vline(xintercept = obs_diff_prop, colour = "blue", linewidth = 1) +
  geom_vline(xintercept = abs(obs_diff_prop), colour = "blue", linewidth = 1)
```


## Python

```{python}
#| results: hide
(ggplot(permuted_stats,
        aes(x = "permuted_diff")) +
     geom_histogram(bins = 6) +
     geom_vline(xintercept = obs_diff_prop, colour = "blue", size = 1) +
     geom_vline(xintercept = abs(obs_diff_prop), colour = "blue", size = 1))
```

:::

#### Statistical evaluation

We can now answer the question if the proportion of rats that make it to full time is different between the groups. We do this by comparing the number of occurrences in the resampled data against the original data. How many times is the difference in proportion larger than the observed difference in proportion? Remember, we are doing a two-tailed test, so we need to get the values on either side of the observed proportion.

In this case the observed difference in proportion between `control` and `treatment` is negative, which we need to take into account.

::: {.panel-tabset group="language"}
## R

```{r}
permuted_stats %>% 
  filter(permuted_diff < obs_diff_prop |
           permuted_diff > abs(obs_diff_prop)) %>% 
  nrow()
```


## Python

```{python}
len(permuted_stats[(permuted_stats['permuted_diff'] < obs_diff_prop) | (permuted_stats['permuted_diff'] > abs(obs_diff_prop))])
```


:::

We find that none of the permuted differences in proportion between `control` and `treatment` that make it to 300s is outside the one we observed. This means that it is extremely unlikely that we'd find these data, if there is indeed no difference between the two groups.

#### Using the median or not

To answer the second question: the median is a particularly useless statistic to use with these data because there is no variation in the measurements for the `control` group. All of the values are 300s, meaning you can't find a value where 50% of the data is on one side of it and 50% of the data is on the other!

#### Number of repetitions

This then brings us to the third question: the number of repetitions. In the worked answer we've used 1,000 repetitions. However, we have two groups with only 12 observations. This means we're limited in the number of unique combinations we can find.

To be more exact, we're limited to:

$C(n, k) = \frac{n!}{k!(n-k)!}$

where

$n!$ (n factorial) is the product of all positive integers up to $n$.
$k!$ (k factorial) is the product of all positive integers up to $k$.

For our data set this comes down to:

$C(2 \times 12, 2) = \frac{(2 \times 12)!}{2!(2 \times 12 - 2)!}$

$C(24, 2) = \frac{24!}{2! \times 22!}$

We can calculate this as follows:

::: {.panel-tabset group="language"}
## R

```{r}
n <- 24
k <- 2

# Calculate the number of unique combinations
factorial(n) / (factorial(k) * factorial(n - k))
```

## Python

```{python}
import math

n = 24
k = 2

math.factorial(n) // (math.factorial(k) * math.factorial(n - k))
```

:::

This means we can create `r factorial(n) / (factorial(k) * factorial(n - k))` unique combinations.

If we run the analysis again using this number, then our output looks like:

```{r}
#| echo: false
#| results: false
set.seed(seed)

# Set the number of permutations
reps <- 276

# Create a place to store the values
permuted_stats <- tibble(permuted_diff = numeric(reps))

# Loop through process
for(i in 1:reps){
  # Get the data  
  permuted_data <- rats_wheel
  
  # Permute (reshuffle) the group labels
  permuted_data$group <- sample(permuted_data$group)
  
  # Calculate the new proportional differences
  
  permuted_diff <- permuted_data %>% 
  group_by(group, time) %>% 
  count() %>% 
  group_by(group) %>% 
  mutate(group_n = sum(n),
         prop_rats = n / group_n) %>% 
  filter(time == 300) %>% 
  pull(prop_rats) %>% 
  diff()
  
  # Store the calculated difference
  permuted_stats$permuted_diff[i] <- permuted_diff
}
```

```{r}
#| echo: false
ggplot(permuted_stats, aes(permuted_diff)) +
  geom_histogram(bins = 6) +
  geom_vline(xintercept = obs_diff_prop, colour = "blue", linewidth = 1) +
  geom_vline(xintercept = abs(obs_diff_prop), colour = "blue", linewidth = 1)
```

This makes a quantitative difference: when we look at the frequency (`count`) of the permuted differences in proportion, it's clear that the numbers are lower than when we permuted the data 1,000 times. This is because when we permuted more times than there were unique combinations, the same answer was calculated multiple times.
:::
:::


<!--
::: {.panel-tabset group="language"}
## R

## Python

:::
-->

<!--
### title {#sec-exr_title}

:::{.callout-exercise}

{{< level 2 >}}

For this exercise we'll be using the data from `data/file.csv`.

::: {.callout-answer collapse="true"}

::: {.panel-tabset group="language"}
## R
Answer

## Python
Answer

:::
:::
:::
-->

## Summary

::: {.callout-tip}
#### Key points

- We can use resampled data to draw conclusions around how likely it is our original data would occur, given the null hypothesis.
- We can calculate statistical significance using this approach.
:::
